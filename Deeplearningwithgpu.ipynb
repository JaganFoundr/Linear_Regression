{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNGs3vk72t6UPTWY6F6qcoH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaganFoundr/PyTorchNN/blob/main/Deeplearningwithgpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LINEAR REGRESSION\n",
        "\n",
        "#importing all the important libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "#inputs (temperature, rainfall, humidity)\n",
        "inputs=np.array([[73,67,45],\n",
        "                 [91,88,64],\n",
        "                 [87,134,58],\n",
        "                 [102,43,37],\n",
        "                 [69,96,70]],dtype='float32')\n",
        "\n",
        "#targets (apples, oranges)\n",
        "targets=np.array([[56,70],\n",
        "                 [81,101],\n",
        "                 [119,133],\n",
        "                 [22,37],\n",
        "                 [103,119]],dtype='float32')\n",
        "\n",
        "#converting inputs and targets from numpy format to tensor format\n",
        "inputs=torch.from_numpy(inputs)\n",
        "targets=torch.from_numpy(targets)\n",
        "\n",
        "# defining random weights and biases\n",
        "weights=torch.randn(2,3,requires_grad=True)\n",
        "bias=torch.randn(2,requires_grad=True)\n",
        "\n",
        "#function for the prediction equation for the model\n",
        "def model(x):\n",
        "  return x @ weights.t()+bias\n",
        "\n",
        "#function for the loss\n",
        "def mse(x,y):\n",
        "  diff=x-y\n",
        "  return torch.sum(diff*diff)/diff.numel()\n",
        "\n",
        "#training loop with 1000 epochs\n",
        "for i in range(1000):\n",
        "\n",
        "  #predicting using the inputs\n",
        "  prediction=model(inputs)\n",
        "\n",
        "  # checking the loss of the prediction\n",
        "  loss=mse(prediction, targets)\n",
        "  print(loss)\n",
        "\n",
        "  # backpropogating to compute gradients and update the weights\n",
        "  loss.backward()\n",
        "\n",
        "  #updating the weights without tracking the gradient\n",
        "  with torch.no_grad():\n",
        "\n",
        "    #learning rate value\n",
        "    lr=0.00001\n",
        "\n",
        "    #updating weights\n",
        "    weights-=weights.grad*lr\n",
        "\n",
        "    #updating bias\n",
        "    bias-=bias.grad*lr\n",
        "\n",
        "    #emptying the gradients of weights and bias\n",
        "    weights.grad.zero_()\n",
        "    bias.grad.zero_()\n",
        "\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "CdEho-pTaSMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MORE COMPLEX LINEAR REGRESSION\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "inputs=np.array([[73,67,43],[91,88,64],[87,134,58],\n",
        "                 [102,43,37],[69,96,70],[73,67,43],\n",
        "                 [91,88,64],[87,134,58],[102,43,37],\n",
        "                 [69,96,70],[73,67,43],[91,88,64],\n",
        "                 [87,134,58],[102,43,37],[69,96,70]],dtype='float32')\n",
        "\n",
        "targets=np.array([[56,70],[81,101],[119,133],\n",
        "                 [22,37],[103,119],[56,70],\n",
        "                 [81,101],[119,133],[22,37],\n",
        "                 [103,119],[56,70],[81,101],\n",
        "                 [119,133],[22,73],[103,119]],dtype='float32')\n",
        "\n",
        "inputs=torch.tensor(inputs)\n",
        "targets=torch.tensor(targets)\n",
        "\n",
        "training_data=TensorDataset(inputs,targets)\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "training_data=DataLoader(training_data, batch_size, shuffle=True )\n",
        "\n",
        "model=nn.Linear(3,2)\n",
        "\n",
        "list(model.parameters())\n",
        "\n",
        "loss_fn = F.mse_loss\n",
        "\n",
        "optimizer=torch.optim.SGD(model.parameters(),lr=0.00001)\n",
        "\n",
        "def train_function(nepochs, model, loss_fn, optimizer):\n",
        "  for epochs in range(nepochs):\n",
        "    for x,y in training_data:\n",
        "\n",
        "      prediction = model(x)\n",
        "\n",
        "      loss = loss_fn(prediction, y)\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      optimizer.step()\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    if (epochs+1)%10==0:\n",
        "      print(f\"epochs: {epochs+1}/{nepochs} , loss: {loss.item()}\")\n",
        "\n",
        "train_function(1000, model, loss_fn, optimizer)\n",
        "\n",
        "prediction=model(inputs)\n",
        "\n",
        "print(prediction)\n",
        "\n",
        "print(targets)"
      ],
      "metadata": {
        "id": "ZJN2ACY-uplj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LOGISTIC REGRESSION USING MNIST\n",
        "\n",
        "#importing libraries\n",
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
        "\n",
        "#main dataset and testdata\n",
        "dataset=MNIST(download=True, train=True, root=\"./data\", transform=transforms.ToTensor())\n",
        "testset=MNIST(root=\"./data\", train=False, transform=transforms.ToTensor())\n",
        "\n",
        "#plotting the dataset\n",
        "image,labels=dataset[1000]\n",
        "plt.imshow(image[0,10:25,10:25], cmap=\"gray\")\n",
        "plt.show()\n",
        "print(\"label: \", labels)"
      ],
      "metadata": {
        "id": "ZQV72q6kWpJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NHGGYphHtljA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the whole dataset into validation data and training data\n",
        "def split_data(dataset, validation_percent):\n",
        "  validation_data=int(dataset*validation_percent)\n",
        "  shuffled=np.random.permutation(dataset)\n",
        "  return shuffled[validation_data:], shuffled[:validation_data]\n",
        "\n",
        "training_data,validation_data = split_data(len(dataset), 0.3)\n",
        "print(\"length of training data: \", len(training_data))\n",
        "print(\"length of validation data: \", len(validation_data))\n",
        "\n",
        "print(\"portion of validation data: \",validation_data[:20])"
      ],
      "metadata": {
        "id": "dlorU70oXLps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#putting all the splitted data to the sampler and then into the dataloader\n",
        "train_data_sampler=SubsetRandomSampler(training_data)\n",
        "valid_data_sampler=SubsetRandomSampler(validation_data)\n",
        "\n",
        "batch_size=100\n",
        "\n",
        "training_loader=DataLoader(dataset, batch_size, sampler=train_data_sampler)\n",
        "validation_loader=DataLoader(dataset, batch_size, sampler=valid_data_sampler)"
      ],
      "metadata": {
        "id": "JIlbxkGddWAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the model\n",
        "input_size=28*28\n",
        "num_classes=10\n",
        "\n",
        "class MNISTmodel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.Linear=nn.Linear(input_size,num_classes)\n",
        "\n",
        "  def forward(self, size):\n",
        "    size=size.reshape(-1,784)\n",
        "    output=self.Linear(size)\n",
        "    return output\n",
        "\n",
        "model=MNISTmodel()"
      ],
      "metadata": {
        "id": "3Gej-4W87N2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#putting the training loader in the for loop as inputs and outputs for prediction\n",
        "for images, labels in training_loader:\n",
        "  prediction=model(images)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zZXbi9a5Pv87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display predictions and sum of the predictions of each array\n",
        "print(prediction[:2])\n",
        "sum=torch.sum(prediction[2])\n",
        "print(sum)\n",
        "\n",
        "#changing the sum of the probablities of the predictions close to 1 and then checking the sum again\n",
        "prob=F.softmax(prediction)\n",
        "print(prob[:2])\n",
        "sum=torch.sum(prob[2])\n",
        "print(sum)\n",
        "\n",
        "#displaying the exact predicted labels by the model\n",
        "max_prob, pred = torch.max(prob, dim=1)\n",
        "print(pred)\n",
        "print(max_prob)\n",
        "\n",
        "#displaying the actual target labels\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "XGqPcJUW9P4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the loss\n",
        "loss_fn=F.cross_entropy"
      ],
      "metadata": {
        "id": "ivMZt7kCgQQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the optimizer\n",
        "opt=torch.optim.SGD(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "iDN3RG9YjUEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy metrics\n",
        "def accuracy(outputs, labels):\n",
        "  _,pred=torch.max(outputs, dim=1)\n",
        "  return (torch.sum(pred==labels).item()/len(pred))*100"
      ],
      "metadata": {
        "id": "bSGzZGi4S59k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss batch function for loss computation, gradient computation, updating weights, resetting gradients, accuracy computation\n",
        "def loss_batch(model, loss_fn, images, labels, opt, metrics=accuracy):\n",
        "  prediction=model(images)\n",
        "  loss=loss_fn(prediction, labels)\n",
        "\n",
        "  if opt is not None:\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "  metric_result=None\n",
        "  if metrics is not None:\n",
        "    metric_result=metrics(prediction, labels)\n",
        "\n",
        "  return loss.item(), len(images), metric_result"
      ],
      "metadata": {
        "id": "oObTH91hPsnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for evaluating the average loss and average accuracy of the validation set\n",
        "def evaluate(model, loss_fn, validation_loader, metrics=accuracy):\n",
        "  with torch.no_grad():\n",
        "    validation_prediction=[loss_batch(model, loss_fn, images, labels, opt=None, metrics=accuracy) for images, labels in validation_loader]\n",
        "\n",
        "    losses, nums, metric=zip(*validation_prediction)\n",
        "\n",
        "    total=np.sum(nums)\n",
        "\n",
        "    average_loss = np.sum(np.multiply(losses, nums))/total\n",
        "\n",
        "    average_metrics=None\n",
        "    if metrics is not None:\n",
        "      average_metrics = np.sum(np.multiply(metric, nums))/total\n",
        "\n",
        "  return average_loss.item(), total, average_metrics"
      ],
      "metadata": {
        "id": "OkoWI6TfbQyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for explicit training\n",
        "def fit(nepochs, model, images, labels, training_loader, validation_loader, opt, metrics=accuracy):\n",
        "  for epoch in range(nepochs):\n",
        "    for images, labels in training_loader:\n",
        "      train_loss,_, train_accuracy=loss_batch(model, loss_fn, images, labels, opt, metrics=accuracy)\n",
        "\n",
        "    valid_loss, _, valid_accuracy= evaluate(model, loss_fn, validation_loader, metrics=accuracy)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}/{nepochs}\")\n",
        "    print(f\"Training loss: {train_loss:.4f} and Validation loss: {valid_loss:.4f}.\")\n",
        "    print(f\"Training accuracy: {train_accuracy:.2f}% and Validation accuracy: {valid_accuracy:.2f}%.\")\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  return train_loss, _, train_accuracy, valid_loss, _, valid_accuracy\n",
        "\n",
        "train_loss,_, train_accuracy, valid_loss, _, valid_accuracy = fit(6, model, images, labels, training_loader, validation_loader, opt, metrics=accuracy)\n",
        "\n",
        "print(\"--\")\n",
        "print(f\"The train accuracy is {train_accuracy:.2f} % and loss is {train_loss:.4f}.\")\n",
        "print(\"--------------------------------------------\")\n",
        "print(f\"The validation accuracy is {valid_accuracy:.2f} % and loss is {valid_loss:.4f}\")"
      ],
      "metadata": {
        "id": "TCttIo85b4ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the model with the testing dataset\n",
        "#function for predicting the test images\n",
        "def predict_image(image, model):\n",
        "  input=image.unsqueeze(0)\n",
        "  output=model(input)\n",
        "  _,preds=torch.max(output, dim=1)\n",
        "\n",
        "  return preds[0].item()"
      ],
      "metadata": {
        "id": "v8W1V8lqiu3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting and displaying different labels\n",
        "image,labels=testset[10]\n",
        "plt.imshow(image[0], cmap=\"gray\")\n",
        "plt.show()\n",
        "print(\"label: \", labels)\n",
        "print(\"predicted: \", predict_image(image, model))\n",
        "\n",
        "image,labels=testset[100]\n",
        "plt.imshow(image[0], cmap=\"gray\")\n",
        "plt.show()\n",
        "print(\"label: \", labels)\n",
        "print(\"predicted: \", predict_image(image, model))\n",
        "\n",
        "image,labels=testset[1000]\n",
        "plt.imshow(image[0], cmap=\"gray\")\n",
        "plt.show()\n",
        "print(\"label: \", labels)\n",
        "print(\"predicted: \", predict_image(image, model))\n",
        "\n",
        "image,labels=testset[905]\n",
        "plt.imshow(image[0], cmap=\"gray\")\n",
        "plt.show()\n",
        "print(\"label: \", labels)\n",
        "print(\"predicted: \", predict_image(image, model))"
      ],
      "metadata": {
        "id": "rmP4znRwkh7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the loss and accuracy on the test set\n",
        "test_loader=DataLoader(testset, batch_size=200)\n",
        "test_loss, total, test_accuracy=evaluate(model, loss_fn, test_loader, metrics=accuracy)\n",
        "print(f\"The test set loss is {test_loss:.4f} and the accuracy is {test_accuracy:.2f}%.\")"
      ],
      "metadata": {
        "id": "xCKU1FQKn-YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving and loading the model\n",
        "torch.save(model.state_dict(),'MNISTlogistic.pth')\n",
        "model.state_dict()"
      ],
      "metadata": {
        "id": "ILIq6GWqqLFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "savedmodel=MNISTmodel()\n",
        "savedmodel.load_state_dict(torch.load('MNISTlogistic.pth'))\n",
        "savedmodel.state_dict()"
      ],
      "metadata": {
        "id": "9s_BDeUdr7ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNhqtZ1sYjVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UPQfPo9TYjST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1\n",
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transform\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import SubsetRandomSampler, DataLoader"
      ],
      "metadata": {
        "id": "wu3GnbgisZBY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "dataset=MNIST(root='./data', download=True, train=True, transform=transform.ToTensor())\n",
        "testset=MNIST(root='./data', download=True, train=False, transform=transform.ToTensor())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "shgd51IfVTNH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "images, labels = dataset[230]\n",
        "plot=images[:,10:17,10:17]\n",
        "plt.imshow(plot[0], cmap='gray')\n",
        "plt.show()\n",
        "print(\"labels: \",labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "YXJRiX_MYaF5",
        "outputId": "4d43d4a8-baef-48b6-fa07-3866ebbce895"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAGdCAYAAAAv9mXmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAWm0lEQVR4nO3df2xV9f348deFrhfEtvwQkI5SNf5ARJhSYQzdpjINUaL+4YzBjDGzRFKmSEwM/wyXZZYlm9FtBMVt6pIx3BZRZ4KMMYEYZfIjJKiJirJYRUAX7C1dcjH0fv9Y7Gd8FcZt77uHWx+P5CTek/fpeV2CfXLu6b3NlUqlUgBAhQ3KegAABiaBASAJgQEgCYEBIAmBASAJgQEgCYEBIAmBASCJmv4+YXd3d+zbty/q6uoil8v19+kB6INSqRSdnZ3R2NgYgwad+Bql3wOzb9++aGpq6u/TAlBB7e3tMX78+BOu6ffA1NXVRUTE7373uzjttNP6+/TJ3H777VmPUHEdHR1ZjwCcoj79Xn4i/R6YT18WO+200wZUYLzcB3yRnMz3PDf5AUhCYABIQmAASEJgAEhCYABIQmAASEJgAEhCYABIQmAASEJgAEhCYABIQmAASEJgAEhCYABIQmAASEJgAEhCYABIQmAASEJgAEiiV4FZsWJFnHXWWTFkyJCYMWNGvPLKK5WeC4AqV3ZgnnzyyViyZEksW7Ysdu7cGVOnTo1rr702Dh48mGI+AKpU2YF54IEH4vvf/34sWLAgJk2aFA8//HCcdtpp8dvf/jbFfABUqbICc+TIkdixY0fMnj37/77AoEExe/bsePnllz/3mGKxGIVC4ZgNgIGvrMB89NFHcfTo0Rg7duwx+8eOHRv79+//3GPa2tqioaGhZ2tqaur9tABUjeQ/RbZ06dLo6Ojo2drb21OfEoBTQE05i88444wYPHhwHDhw4Jj9Bw4ciDPPPPNzj8nn85HP53s/IQBVqawrmNra2pg2bVps3LixZ193d3ds3LgxZs6cWfHhAKheZV3BREQsWbIk5s+fHy0tLTF9+vR48MEHo6urKxYsWJBiPgCqVNmBueWWW+LDDz+MH/7wh7F///74yle+Es8///xnbvwD8MVWdmAiIhYtWhSLFi2q9CwADCA+iwyAJAQGgCQEBoAkBAaAJAQGgCQEBoAkBAaAJAQGgCQEBoAkBAaAJAQGgCQEBoAkBAaAJAQGgCQEBoAkBAaAJAQGgCQEBoAkBAaAJHKlUqnUnycsFArR0NAQw4cPj1wu15+nTurQoUNZjwDQbzo6OqK+vv6Ea1zBAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJBE2YHZsmVLzJ07NxobGyOXy8XTTz+dYCwAql3Zgenq6oqpU6fGihUrUswDwABRU+4Bc+bMiTlz5qSYBYABpOzAlKtYLEaxWOx5XCgUUp8SgFNA8pv8bW1t0dDQ0LM1NTWlPiUAp4DkgVm6dGl0dHT0bO3t7alPCcApIPlLZPl8PvL5fOrTAHCK8T4YAJIo+wrm8OHDsWfPnp7He/fujV27dsXIkSNjwoQJFR0OgOqVK5VKpXIO2LRpU1x55ZWf2T9//vx4/PHH/+fxhUIhGhoaYvjw4ZHL5co59Snt0KFDWY8A0G86Ojqivr7+hGvKvoL55je/GWU2CYAvIPdgAEhCYABIQmAASEJgAEhCYABIQmAASEJgAEhCYABIQmAASEJgAEhCYABIQmAASEJgAEhCYABIQmAASEJgAEhCYABIQmAASEJgAEiiJqsTf/zxx1mdGqBf5fP5rEeomFKpFEeOHDmpta5gAEhCYABIQmAASEJgAEhCYABIQmAASEJgAEhCYABIQmAASEJgAEhCYABIQmAASEJgAEhCYABIQmAASEJgAEhCYABIQmAASEJgAEhCYABIQmAASEJgAEiirMC0tbXFZZddFnV1dTFmzJi48cYb44033kg1GwBVrKzAbN68OVpbW2Pr1q2xYcOG+OSTT+Kaa66Jrq6uVPMBUKVypVKp1NuDP/zwwxgzZkxs3rw5vv71r5/UMYVCIRoaGnp7SoCqk8/nsx6hYkqlUhw5ciQ6Ojqivr7+hGtr+nKijo6OiIgYOXLkcdcUi8UoFos9jwuFQl9OCUCV6PVN/u7u7li8eHHMmjUrJk+efNx1bW1t0dDQ0LM1NTX19pQAVJFev0S2cOHCWLduXbz44osxfvz44677vCsYkQG+SLxEVoZFixbFc889F1u2bDlhXCL+8wc7kP5wATg5ZQWmVCrFD37wg1i7dm1s2rQpzj777FRzAVDlygpMa2trrF69Op555pmoq6uL/fv3R0REQ0NDDB06NMmAAFSnsu7B5HK5z93/2GOPxXe/+92T+hp+TBn4ohlItwmS3YPpw1tmAPiC8VlkACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJFHWr0wGoHz33HNP1iNUTLFYjJ/97GcntdYVDABJCAwASQgMAEkIDABJCAwASQgMAEkIDABJCAwASQgMAEkIDABJCAwASQgMAEkIDABJCAwASQgMAEkIDABJCAwASQgMAEkIDABJCAwASQgMAEkIDABJlBWYlStXxpQpU6K+vj7q6+tj5syZsW7dulSzAVDFygrM+PHjY/ny5bFjx47Yvn17XHXVVXHDDTfEa6+9lmo+AKpUTTmL586de8zjn/zkJ7Fy5crYunVrXHTRRRUdDIDqVlZg/tvRo0fjT3/6U3R1dcXMmTOPu65YLEaxWOx5XCgUentKAKpI2Tf5d+/eHaeffnrk8/m44447Yu3atTFp0qTjrm9ra4uGhoaerampqU8DA1Adyg7MBRdcELt27Yp//OMfsXDhwpg/f368/vrrx12/dOnS6Ojo6Nna29v7NDAA1aHsl8hqa2vj3HPPjYiIadOmxbZt2+Khhx6KRx555HPX5/P5yOfzfZsSgKrT5/fBdHd3H3OPBQAiyryCWbp0acyZMycmTJgQnZ2dsXr16ti0aVOsX78+1XwAVKmyAnPw4MH4zne+Ex988EE0NDTElClTYv369fGtb30r1XwAVKmyAvOb3/wm1RwADDA+iwyAJAQGgCQEBoAkBAaAJAQGgCQEBoAkBAaAJAQGgCQEBoAkBAaAJAQGgCQEBoAkBAaAJAQGgCQEBoAkBAaAJAQGgCQEBoAkBAaAJHKlUqnUnycsFArR0NDQn6cEqshXv/rVrEeouHXr1mU9QsUUCoVobm6Ojo6OqK+vP+FaVzAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJNGnwCxfvjxyuVwsXry4QuMAMFD0OjDbtm2LRx55JKZMmVLJeQAYIHoVmMOHD8e8efPi0UcfjREjRlR6JgAGgF4FprW1Na677rqYPXv2/1xbLBajUCgcswEw8NWUe8CaNWti586dsW3btpNa39bWFj/60Y/KHgyA6lbWFUx7e3vcdddd8fvf/z6GDBlyUscsXbo0Ojo6erb29vZeDQpAdSnrCmbHjh1x8ODBuPTSS3v2HT16NLZs2RK/+tWvolgsxuDBg485Jp/PRz6fr8y0AFSNsgJz9dVXx+7du4/Zt2DBgpg4cWLce++9n4kLAF9cZQWmrq4uJk+efMy+YcOGxahRoz6zH4AvNu/kByCJsn+K7P+3adOmCowBwEDjCgaAJAQGgCQEBoAkBAaAJAQGgCQEBoAkBAaAJAQGgCQEBoAkBAaAJAQGgCQEBoAkBAaAJAQGgCQEBoAkBAaAJAQGgCQEBoAkBAaAJGqyHgDgv23YsCHrESpu2LBhWY9QMblc7qTXuoIBIAmBASAJgQEgCYEBIAmBASAJgQEgCYEBIAmBASAJgQEgCYEBIAmBASAJgQEgCYEBIAmBASAJgQEgCYEBIAmBASAJgQEgCYEBIAmBASAJgQEgCYEBIImyAnPfffdFLpc7Zps4cWKq2QCoYjXlHnDRRRfF3/72t//7AjVlfwkAvgDKrkNNTU2ceeaZKWYBYAAp+x7MW2+9FY2NjXHOOefEvHnz4t133z3h+mKxGIVC4ZgNgIGvrMDMmDEjHn/88Xj++edj5cqVsXfv3rjiiiuis7PzuMe0tbVFQ0NDz9bU1NTnoQE49eVKpVKptwd//PHH0dzcHA888EDcfvvtn7umWCxGsVjseVwoFEQGOK4T/YO1Wg0bNizrESqmUCjE8OHDo6OjI+rr60+4tk936IcPHx7nn39+7Nmz57hr8vl85PP5vpwGgCrUp/fBHD58ON5+++0YN25cpeYBYIAoKzD33HNPbN68Of75z3/GSy+9FDfddFMMHjw4br311lTzAVClynqJ7L333otbb701/vWvf8Xo0aPj8ssvj61bt8bo0aNTzQdAlSorMGvWrEk1BwADjM8iAyAJgQEgCYEBIAmBASAJgQEgCYEBIAmBASAJgQEgCYEBIAmBASAJgQEgCYEBIAmBASAJgQEgCYEBIAmBASAJgQEgCYEBIAmBASCJmqwHAHrvqaeeynqEijv99NOzHqHiOjs7sx6hYg4fPnzSa13BAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJCEwACQhMAAkITAAJBE2YF5//3347bbbotRo0bF0KFD4+KLL47t27enmA2AKlZTzuJDhw7FrFmz4sorr4x169bF6NGj46233ooRI0akmg+AKlVWYH76059GU1NTPPbYYz37zj777IoPBUD1K+slsmeffTZaWlri5ptvjjFjxsQll1wSjz766AmPKRaLUSgUjtkAGPjKCsw777wTK1eujPPOOy/Wr18fCxcujDvvvDOeeOKJ4x7T1tYWDQ0NPVtTU1Ofhwbg1JcrlUqlk11cW1sbLS0t8dJLL/Xsu/POO2Pbtm3x8ssvf+4xxWIxisViz+NCoSAyUCFPPfVU1iNU3E033ZT1CBXX2dmZ9QgVUygUYvz48dHR0RH19fUnXFvWFcy4ceNi0qRJx+y78MIL49133z3uMfl8Purr64/ZABj4ygrMrFmz4o033jhm35tvvhnNzc0VHQqA6ldWYO6+++7YunVr3H///bFnz55YvXp1rFq1KlpbW1PNB0CVKiswl112Waxduzb+8Ic/xOTJk+PHP/5xPPjggzFv3rxU8wFQpcp6H0xExPXXXx/XX399ilkAGEB8FhkASQgMAEkIDABJCAwASQgMAEkIDABJCAwASQgMAEkIDABJCAwASQgMAEkIDABJCAwASQgMAEkIDABJCAwASQgMAEkIDABJlP0rk/uqVCr19ylhwPr3v/+d9QgVVygUsh6h4jo7O7MeoWI+fS4n8708V+rn7/jvvfdeNDU19ecpAaiw9vb2GD9+/AnX9Htguru7Y9++fVFXVxe5XC7ZeQqFQjQ1NUV7e3vU19cnO09/8pxOfQPt+UR4TtWiv55TqVSKzs7OaGxsjEGDTnyXpd9fIhs0aND/rF4l1dfXD5i/QJ/ynE59A+35RHhO1aI/nlNDQ8NJrXOTH4AkBAaAJAZsYPL5fCxbtizy+XzWo1SM53TqG2jPJ8Jzqhan4nPq95v8AHwxDNgrGACyJTAAJCEwACQhMAAkMSADs2LFijjrrLNiyJAhMWPGjHjllVeyHqlPtmzZEnPnzo3GxsbI5XLx9NNPZz1Sn7S1tcVll10WdXV1MWbMmLjxxhvjjTfeyHqsPlm5cmVMmTKl501uM2fOjHXr1mU9VkUtX748crlcLF68OOtReu2+++6LXC53zDZx4sSsx+qT999/P2677bYYNWpUDB06NC6++OLYvn171mNFxAAMzJNPPhlLliyJZcuWxc6dO2Pq1Klx7bXXxsGDB7Merde6urpi6tSpsWLFiqxHqYjNmzdHa2trbN26NTZs2BCffPJJXHPNNdHV1ZX1aL02fvz4WL58eezYsSO2b98eV111Vdxwww3x2muvZT1aRWzbti0eeeSRmDJlStaj9NlFF10UH3zwQc/24osvZj1Srx06dChmzZoVX/rSl2LdunXx+uuvx89//vMYMWJE1qP9R2mAmT59eqm1tbXn8dGjR0uNjY2ltra2DKeqnIgorV27NusxKurgwYOliCht3rw561EqasSIEaVf//rXWY/RZ52dnaXzzjuvtGHDhtI3vvGN0l133ZX1SL22bNmy0tSpU7Meo2Luvffe0uWXX571GMc1oK5gjhw5Ejt27IjZs2f37Bs0aFDMnj07Xn755Qwn40Q6OjoiImLkyJEZT1IZR48ejTVr1kRXV1fMnDkz63H6rLW1Na677rpj/r+qZm+99VY0NjbGOeecE/PmzYt3330365F67dlnn42Wlpa4+eabY8yYMXHJJZfEo48+mvVYPQZUYD766KM4evRojB079pj9Y8eOjf3792c0FSfS3d0dixcvjlmzZsXkyZOzHqdPdu/eHaeffnrk8/m44447Yu3atTFp0qSsx+qTNWvWxM6dO6OtrS3rUSpixowZ8fjjj8fzzz8fK1eujL1798YVV1xRtb+v5Z133omVK1fGeeedF+vXr4+FCxfGnXfeGU888UTWo0VEBp+mDP+ttbU1Xn311ap+HfxTF1xwQezatSs6Ojriz3/+c8yfPz82b95ctZFpb2+Pu+66KzZs2BBDhgzJepyKmDNnTs9/T5kyJWbMmBHNzc3xxz/+MW6//fYMJ+ud7u7uaGlpifvvvz8iIi655JJ49dVX4+GHH4758+dnPN0Au4I544wzYvDgwXHgwIFj9h84cCDOPPPMjKbieBYtWhTPPfdcvPDCC/36KxxSqa2tjXPPPTemTZsWbW1tMXXq1HjooYeyHqvXduzYEQcPHoxLL700ampqoqamJjZv3hy/+MUvoqamJo4ePZr1iH02fPjwOP/882PPnj1Zj9Ir48aN+8w/YC688MJT5mW/ARWY2tramDZtWmzcuLFnX3d3d2zcuHFAvBY+UJRKpVi0aFGsXbs2/v73v8fZZ5+d9UhJdHd3R7FYzHqMXrv66qtj9+7dsWvXrp6tpaUl5s2bF7t27YrBgwdnPWKfHT58ON5+++0YN25c1qP0yqxZsz7zI/5vvvlmNDc3ZzTRsQbcS2RLliyJ+fPnR0tLS0yfPj0efPDB6OrqigULFmQ9Wq8dPnz4mH9h7d27N3bt2hUjR46MCRMmZDhZ77S2tsbq1avjmWeeibq6up77Yw0NDTF06NCMp+udpUuXxpw5c2LChAnR2dkZq1evjk2bNsX69euzHq3X6urqPnNfbNiwYTFq1KiqvV92zz33xNy5c6O5uTn27dsXy5Yti8GDB8ett96a9Wi9cvfdd8fXvva1uP/+++Pb3/52vPLKK7Fq1apYtWpV1qP9R9Y/xpbCL3/5y9KECRNKtbW1penTp5e2bt2a9Uh98sILL5Qi4jPb/Pnzsx6tVz7vuURE6bHHHst6tF773ve+V2pubi7V1taWRo8eXbr66qtLf/3rX7Meq+Kq/ceUb7nlltK4ceNKtbW1pS9/+culW265pbRnz56sx+qTv/zlL6XJkyeX8vl8aeLEiaVVq1ZlPVIPH9cPQBID6h4MAKcOgQEgCYEBIAmBASAJgQEgCYEBIAmBASAJgQEgCYEBIAmBASAJgQEgCYEBIIn/B3N4Vfy9HUM5AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels:  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "def splitted_data(data, valid_data_percent):\n",
        "  valid_num=int(data*valid_data_percent)\n",
        "  index=np.random.permutation(data)\n",
        "  return index[valid_num:], index[:valid_num]\n",
        "training_data, validation_data=splitted_data(len(dataset), 0.25)\n",
        "print(\"Training data: \",len(training_data))\n",
        "print(\"Validation data\",len(validation_data))\n",
        "print(\"portion of validation data: \", validation_data[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjY91nW4YZ3Z",
        "outputId": "5ed0b668-9726-4c2f-ecd7-ae095b7c9239"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data:  45000\n",
            "Validation data 15000\n",
            "portion of validation data:  [33785 57614 26916 15407 24403 56920 12757 21044 37272 21664 15867 10383\n",
            " 17940  4566 12815 35714 13302 40611  3304 52190]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "training_sampler=SubsetRandomSampler(training_data)\n",
        "validation_sampler=SubsetRandomSampler(validation_data)\n",
        "\n",
        "batch_size=100\n",
        "\n",
        "training_loader=DataLoader(dataset=dataset, batch_size=batch_size, sampler=training_sampler)\n",
        "validation_loader=DataLoader(dataset=dataset, batch_size=batch_size, sampler=validation_sampler)"
      ],
      "metadata": {
        "id": "1c2jumtnYZ0s"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6\n",
        "input_size=28*28\n",
        "hidden_size1=256\n",
        "hidden_size2=128\n",
        "output_size=10\n",
        "class MNISTMODEL(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size1, hidden_size2 ,output_size):\n",
        "    super().__init__()\n",
        "    #hidden layer1\n",
        "    self.linear1=nn.Linear(input_size, hidden_size1)\n",
        "    #hidden layer2\n",
        "    self.linear2=nn.Linear(hidden_size1, hidden_size2)\n",
        "    #output layer\n",
        "    self.linear3=nn.Linear(hidden_size2, output_size)\n",
        "\n",
        "\n",
        "  def forward(self,batch):\n",
        "    size=batch.view(batch.size(0),-1)\n",
        "\n",
        "    hidden1=self.linear1(size)\n",
        "\n",
        "    output=F.relu(hidden1)\n",
        "\n",
        "    hidden2=self.linear2(output)\n",
        "\n",
        "    output=F.relu(hidden2)\n",
        "\n",
        "    output=self.linear3(output)\n",
        "\n",
        "    return output\n",
        "\n",
        "model=MNISTMODEL(input_size, hidden_size1, hidden_size2, output_size).to(device)"
      ],
      "metadata": {
        "id": "zlKTAD2HYZyT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in model.parameters():\n",
        "  print(t.shape)\n",
        "  print(t.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M5CWRypoUj6",
        "outputId": "af1a6172-4492-41bc-d8ca-8b8cf1262df9"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 784])\n",
            "cuda:0\n",
            "torch.Size([256])\n",
            "cuda:0\n",
            "torch.Size([128, 256])\n",
            "cuda:0\n",
            "torch.Size([128])\n",
            "cuda:0\n",
            "torch.Size([10, 128])\n",
            "cuda:0\n",
            "torch.Size([10])\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7\n",
        "for images, labels in training_loader:\n",
        "  images, labels = images.to(device), labels.to(device)\n",
        "  prediction=model(images)"
      ],
      "metadata": {
        "id": "4vmTOP1aYZue"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8\n",
        "print(prediction[0])\n",
        "\n",
        "#9\n",
        "prob_sum=torch.sum(prediction[0])\n",
        "print(prob_sum)\n",
        "\n",
        "changed_pred=F.softmax(prediction)\n",
        "changed_sum=torch.sum(changed_pred[9])\n",
        "print(changed_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26sKMZg6YZjv",
        "outputId": "de2e40d4-9460-44ee-8bc8-4a57158bed0f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 5.1349, -4.3022,  0.5593, -0.0089, -2.1328,  2.4256,  1.2476, -3.1976,\n",
            "         0.7817, -2.8177], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "tensor(-2.3101, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(1., device='cuda:0', grad_fn=<SumBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-a0f466636ccd>:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  changed_pred=F.softmax(prediction)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10\n",
        "_,pred=torch.max(prediction, dim=1)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8PPAvBlgqZA",
        "outputId": "6114966c-3eb4-44f7-e515-939adbf00a9c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 4, 3, 1, 7, 5, 1, 8, 6, 9, 8, 3, 9, 3, 1, 7, 3, 6, 0, 2, 3, 5, 5, 9,\n",
            "        9, 9, 2, 5, 1, 8, 4, 5, 9, 7, 3, 7, 9, 1, 4, 4, 5, 5, 5, 2, 0, 3, 0, 0,\n",
            "        0, 7, 0, 7, 1, 1, 2, 3, 7, 1, 2, 1, 1, 6, 5, 4, 8, 2, 4, 1, 0, 3, 6, 9,\n",
            "        1, 1, 0, 0, 6, 1, 1, 9, 6, 3, 8, 0, 6, 6, 4, 3, 9, 6, 2, 7, 9, 1, 1, 1,\n",
            "        2, 2, 6, 5], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11\n",
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ydw4qipQgqRs",
        "outputId": "56e08a4a-e34f-4df8-ddae-669cf1eea8c4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 4, 3, 1, 7, 5, 1, 5, 6, 9, 8, 3, 9, 3, 1, 7, 3, 6, 0, 2, 5, 5, 5, 4,\n",
              "        9, 9, 2, 5, 1, 3, 4, 5, 9, 3, 8, 7, 9, 1, 9, 4, 5, 5, 5, 2, 0, 3, 0, 0,\n",
              "        0, 7, 0, 7, 1, 1, 2, 3, 7, 7, 2, 1, 5, 6, 3, 9, 8, 2, 4, 1, 0, 3, 0, 8,\n",
              "        2, 1, 0, 0, 6, 8, 7, 7, 4, 3, 8, 0, 5, 0, 4, 8, 9, 6, 2, 2, 9, 1, 8, 8,\n",
              "        2, 2, 6, 5], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12\n",
        "def accuracy(output, labels):\n",
        "  _,pred=torch.max(output, dim=1)\n",
        "  return torch.sum(pred==labels).item()/len(pred)*100"
      ],
      "metadata": {
        "id": "AYn7pl5AYZaC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13\n",
        "loss_function=F.cross_entropy"
      ],
      "metadata": {
        "id": "whNpBd-Vfcy3"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14\n",
        "opt=torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "5OhakYczfcwQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15\n",
        "def loss_batch(model, loss_function, images, labels, opt, metrics=accuracy):\n",
        "  prediction=model(images.to(device))\n",
        "  loss=loss_function(prediction, labels.to(device))\n",
        "\n",
        "  if opt is not None:\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "  metric_result=None\n",
        "  if metrics is not None:\n",
        "    metric_result=metrics(prediction, labels.to(device))\n",
        "\n",
        "  return loss.item(), len(images), metric_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0su4yJQfct6",
        "outputId": "01702a5f-4cfc-4914-ac1c-a662f42c5fa6"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16\n",
        "def evaluate(model, loss_function, validation_loader, metrics=accuracy):\n",
        "  with torch.no_grad():\n",
        "    result=[loss_batch(model, loss_function, images.to(device), labels.to(device), opt=None, metrics=accuracy) for images, labels in validation_loader]\n",
        "\n",
        "    losses, num, metric=zip(*result)\n",
        "\n",
        "    total=np.sum(num)\n",
        "\n",
        "    loss=np.sum(np.multiply(losses, num))/total\n",
        "\n",
        "    if metrics is not None:\n",
        "      metric=np.sum(np.multiply(metric, num))/total\n",
        "  return loss, total, metric"
      ],
      "metadata": {
        "id": "3mu76XCEfcrj"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17\n",
        "def train(nepochs, model, loss_function, training_loader, validation_loader, images, labels, opt, metrics=accuracy):\n",
        "  for epoch in range(nepochs):\n",
        "    for images, labels in training_loader:\n",
        "\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "      train_loss,_,train_acc=loss_batch(model, loss_function, images.to(device), labels.to(device), opt, metrics=accuracy)\n",
        "\n",
        "    valid_loss,_,valid_acc=evaluate(model, loss_function, validation_loader, metrics=accuracy)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1}/{nepochs}\")\n",
        "    print(f\"Training loss: {train_loss:.4f} and Validation loss: {valid_loss:.4f}.\")\n",
        "    print(f\"Training accuracy: {train_acc:.2f}% and Validation accuracy: {valid_acc:.2f}%.\")\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  return train_loss,train_acc, valid_loss,valid_acc\n",
        "\n",
        "train_loss, train_acc, valid_loss, valid_acc = train(1, model, loss_function, training_loader, validation_loader, images, labels, opt, metrics=accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI5bHKeEfco7",
        "outputId": "cfc4c5f2-ab25-4b8b-d564-bacb747b2d65"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/1\n",
            "Training loss: 0.2371 and Validation loss: 0.2243.\n",
            "Training accuracy: 95.00% and Validation accuracy: 93.61%.\n",
            "--------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18\n",
        "def prediction(images, model):\n",
        "  input=images.to(device).unsqueeze(0)\n",
        "  output=model(input)\n",
        "  _,pred=torch.max(output, dim=1)\n",
        "\n",
        "  return pred[0].item()"
      ],
      "metadata": {
        "id": "cZ2_ceTcfcmZ"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19\n",
        "images, labels=testset[6]\n",
        "plt.imshow(images[0], cmap='gray')\n",
        "plt.show()\n",
        "print(\"labels: \", labels)\n",
        "print(\"predicted: \",prediction(images.to(device), model))\n",
        "\n",
        "test_loader=DataLoader(testset, batch_size=200)\n",
        "\n",
        "\n",
        "#test accuracy (you define it in your own form)\n",
        "print(evaluate(model, loss_function, test_loader, metrics=accuracy))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "CfzFPJNqfcjs",
        "outputId": "818a7898-f6f9-40a9-c2c6-3a192fc2a1ec"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "labels:  4\n",
            "predicted:  4\n",
            "(0.21343292355537413, 10000, 93.78)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20\n",
        "torch.save(model.state_dict(), \"MNIST.pth\")\n",
        "model.state_dict()"
      ],
      "metadata": {
        "id": "p9Tq1Ttsfbyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model=MNISTMODEL(input_size, hidden_size1, hidden_size2, output_size)\n",
        "saved_model.load_state_dict(torch.load('MNIST.pth'))\n",
        "saved_model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgEx63M7ouwh",
        "outputId": "1b21cc7e-dcc9-4b15-f01c-57ac8907537d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-450330895b00>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  saved_model.load_state_dict(torch.load('MNIST.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear1.weight',\n",
              "              tensor([[ 0.0309, -0.0147, -0.0004,  ..., -0.0085,  0.0245, -0.0021],\n",
              "                      [-0.0130, -0.0012,  0.0206,  ..., -0.0162, -0.0222,  0.0155],\n",
              "                      [ 0.0034, -0.0287, -0.0199,  ...,  0.0281,  0.0024, -0.0180],\n",
              "                      ...,\n",
              "                      [-0.0086, -0.0159, -0.0022,  ..., -0.0001,  0.0351, -0.0111],\n",
              "                      [-0.0244, -0.0195,  0.0345,  ..., -0.0222, -0.0320,  0.0198],\n",
              "                      [-0.0317, -0.0181,  0.0255,  ..., -0.0042, -0.0212,  0.0119]])),\n",
              "             ('linear1.bias',\n",
              "              tensor([ 3.1300e-02,  4.4668e-02,  1.5617e-02,  1.8764e-02,  7.5081e-02,\n",
              "                       3.0093e-02, -1.8613e-02, -1.1791e-02, -4.8617e-03,  6.2139e-02,\n",
              "                      -1.3675e-03,  5.0789e-02, -4.3445e-02, -4.8329e-04,  4.3572e-03,\n",
              "                       2.2874e-02, -4.8038e-05, -8.8091e-03,  9.5547e-04,  1.8695e-02,\n",
              "                       3.4536e-02,  3.1684e-02, -1.5120e-03,  2.5258e-02, -1.3219e-02,\n",
              "                       1.5424e-02,  3.8381e-02, -1.5327e-02,  2.8312e-02,  3.5237e-03,\n",
              "                       3.3000e-02,  3.1404e-02, -3.8400e-02,  2.7971e-04,  2.6963e-02,\n",
              "                       2.3869e-03,  3.5472e-02,  2.5999e-03,  5.4101e-02,  4.3444e-02,\n",
              "                       3.4469e-02,  2.6826e-02,  3.1174e-02,  1.2653e-02, -1.8914e-02,\n",
              "                      -2.4741e-02,  1.7945e-02,  1.2408e-02,  2.5572e-03, -2.7390e-02,\n",
              "                       3.3883e-03, -1.2373e-02, -3.1246e-03, -1.7508e-02,  2.9219e-03,\n",
              "                       6.7000e-02, -1.7690e-02, -2.9594e-02, -1.3431e-02, -2.9810e-02,\n",
              "                       5.8251e-02,  1.4077e-02,  1.8054e-02,  1.7867e-02,  3.1247e-03,\n",
              "                      -1.1899e-02,  1.6176e-02,  1.8433e-02,  2.1607e-02,  1.3164e-02,\n",
              "                       4.9528e-02, -2.8842e-03,  3.2954e-02,  4.0818e-02,  1.0200e-02,\n",
              "                      -3.2594e-03,  1.1853e-02,  2.9393e-02,  5.4860e-03,  2.2657e-02,\n",
              "                      -4.6796e-03,  3.6237e-02, -2.1372e-02,  4.8607e-02,  2.3330e-02,\n",
              "                      -9.0065e-03,  6.6906e-02, -1.8633e-03,  6.2638e-02,  2.6532e-02,\n",
              "                       1.7915e-03,  1.2640e-02,  2.7376e-02,  1.7805e-03, -5.9608e-03,\n",
              "                       7.5799e-03, -1.3630e-02, -2.4104e-02, -1.1154e-02,  8.9888e-03,\n",
              "                      -1.1695e-02,  3.2910e-03, -9.8046e-04, -2.3687e-03, -8.4281e-03,\n",
              "                       2.6657e-02,  3.1641e-02,  2.2554e-02,  5.8454e-02,  1.0853e-02,\n",
              "                      -1.6064e-03,  2.3368e-02,  2.1805e-02,  7.9496e-03,  5.7527e-02,\n",
              "                       5.8119e-02,  9.4656e-03,  1.8030e-02,  1.2977e-02, -2.2349e-02,\n",
              "                      -6.9033e-03,  3.0121e-02,  2.5056e-02,  5.3673e-02, -1.3923e-02,\n",
              "                       1.5385e-02,  2.8895e-03,  3.1350e-02,  1.6887e-02, -9.3610e-03,\n",
              "                      -1.1166e-02,  3.6582e-02, -9.0971e-03, -1.6133e-02, -4.3950e-03,\n",
              "                       3.1403e-02,  8.8958e-02,  4.0766e-02,  9.0449e-04, -5.2000e-03,\n",
              "                       3.3253e-02,  4.4971e-02,  1.2372e-02, -2.5110e-02, -9.3445e-03,\n",
              "                       5.5238e-03, -2.3112e-03,  3.0232e-02, -3.6991e-03, -5.7369e-02,\n",
              "                       9.7862e-03,  1.2303e-02,  1.2834e-02,  4.6157e-02, -4.7057e-03,\n",
              "                       2.4245e-02, -4.0562e-03,  3.8199e-02,  1.7623e-02, -6.5337e-03,\n",
              "                      -8.9164e-03,  3.1517e-03, -1.0335e-02, -4.0038e-03,  1.4224e-02,\n",
              "                      -1.2738e-02,  4.5772e-03,  1.2882e-02,  1.7224e-02, -1.3284e-02,\n",
              "                      -2.6071e-02,  2.7573e-02,  3.8827e-02, -2.2734e-02,  5.4975e-02,\n",
              "                       1.6186e-02,  2.9165e-02,  4.4567e-02,  2.2472e-02, -1.6668e-02,\n",
              "                      -2.2445e-02,  3.0197e-03,  2.8660e-03,  2.8444e-02, -1.1127e-02,\n",
              "                       2.2759e-02,  2.0446e-02, -3.0901e-02,  4.1147e-02,  1.8969e-02,\n",
              "                       3.5989e-02,  3.3546e-02,  4.3881e-02, -1.2908e-02,  2.4537e-02,\n",
              "                      -2.0864e-02, -2.7773e-02,  1.7871e-02,  7.9657e-03,  2.5141e-03,\n",
              "                      -1.1684e-02, -1.7226e-02, -7.5098e-03,  4.0772e-03,  1.1397e-02,\n",
              "                      -2.6828e-02, -2.6750e-02,  4.8564e-03, -5.2550e-03,  2.3433e-02,\n",
              "                      -5.0912e-04, -5.8128e-03,  1.0589e-02,  3.4642e-03,  3.0865e-03,\n",
              "                       1.0881e-02,  2.5855e-03, -9.6244e-03,  1.8612e-02, -6.3014e-03,\n",
              "                       7.2749e-02,  3.4298e-02,  1.2446e-02,  5.1609e-02,  3.5244e-02,\n",
              "                      -1.4832e-03, -1.4833e-02,  3.9902e-02, -3.3306e-03, -2.3232e-02,\n",
              "                       3.2726e-02,  1.5478e-02,  3.8977e-02, -1.3996e-02,  3.2112e-02,\n",
              "                       4.6060e-02, -2.7657e-02,  2.0260e-03,  4.0493e-02, -1.8305e-02,\n",
              "                       4.7943e-02, -2.4611e-02,  4.3990e-02, -1.9809e-02,  2.7781e-02,\n",
              "                      -3.1152e-02, -8.9023e-03,  3.4385e-02, -7.9958e-04, -5.4336e-03,\n",
              "                       5.6279e-03,  4.2387e-02,  1.2974e-02,  8.4533e-03,  4.0302e-02,\n",
              "                      -1.7264e-02])),\n",
              "             ('linear2.weight',\n",
              "              tensor([[-0.0186, -0.0099,  0.0490,  ..., -0.0438,  0.0123, -0.0448],\n",
              "                      [-0.0043,  0.0246,  0.0066,  ...,  0.0266, -0.0406, -0.0535],\n",
              "                      [ 0.0606,  0.0427,  0.0554,  ..., -0.0104, -0.0073,  0.0510],\n",
              "                      ...,\n",
              "                      [ 0.0561,  0.1121, -0.0279,  ..., -0.0345, -0.0464, -0.0217],\n",
              "                      [-0.0029,  0.0203,  0.0415,  ...,  0.0784, -0.0160,  0.0027],\n",
              "                      [ 0.0509, -0.0426, -0.0087,  ...,  0.0157,  0.0180,  0.0369]])),\n",
              "             ('linear2.bias',\n",
              "              tensor([-0.0035,  0.0083, -0.0166, -0.0502, -0.0510,  0.0565, -0.0075,  0.0357,\n",
              "                       0.0227,  0.0498, -0.0324, -0.0316, -0.0422,  0.0147, -0.0047, -0.0166,\n",
              "                      -0.0316,  0.0776, -0.0534,  0.0251,  0.0454, -0.0063, -0.0546, -0.0681,\n",
              "                       0.0847,  0.0204,  0.0330,  0.0519,  0.0481, -0.0634,  0.0365, -0.0385,\n",
              "                      -0.0605, -0.0032,  0.0330,  0.0501, -0.0303, -0.0156,  0.0093,  0.0131,\n",
              "                      -0.0858,  0.0261,  0.0513,  0.0905,  0.0076,  0.0441,  0.0063,  0.0328,\n",
              "                      -0.0335,  0.0192,  0.0032,  0.0577,  0.0881,  0.0351, -0.0431,  0.0105,\n",
              "                       0.0263,  0.0753,  0.0631,  0.0614,  0.0768,  0.0364,  0.0651,  0.0069,\n",
              "                      -0.0247,  0.0035,  0.0113, -0.0058, -0.0380, -0.0353, -0.0597, -0.0412,\n",
              "                      -0.0388,  0.0793,  0.0358,  0.0523,  0.0254, -0.0391, -0.0163,  0.0803,\n",
              "                       0.0277,  0.0331, -0.0104,  0.0342,  0.1109, -0.0046, -0.0359,  0.0353,\n",
              "                      -0.0369,  0.0861,  0.0907, -0.0493, -0.0237,  0.0363, -0.0193,  0.1013,\n",
              "                       0.0378,  0.0170,  0.0049,  0.0406, -0.0305, -0.0227,  0.0850,  0.0116,\n",
              "                       0.0711,  0.0862,  0.0694, -0.0314,  0.0431,  0.0159, -0.0146,  0.0022,\n",
              "                       0.0464,  0.0451,  0.0349,  0.0890,  0.0021,  0.0150, -0.0145,  0.0490,\n",
              "                      -0.0484,  0.0679, -0.0345,  0.0442,  0.0972,  0.0649,  0.0765, -0.0307])),\n",
              "             ('linear3.weight',\n",
              "              tensor([[-0.0707, -0.0133,  0.0185,  ..., -0.2258, -0.1576, -0.0962],\n",
              "                      [-0.1328, -0.0961,  0.0518,  ..., -0.2019, -0.0448,  0.1603],\n",
              "                      [ 0.1161, -0.1627, -0.0339,  ...,  0.0598,  0.0801, -0.0587],\n",
              "                      ...,\n",
              "                      [-0.1766,  0.0844, -0.0004,  ...,  0.2951, -0.0714,  0.0580],\n",
              "                      [ 0.1224,  0.0200,  0.0678,  ...,  0.1297, -0.0595, -0.0491],\n",
              "                      [-0.0462,  0.0708,  0.0522,  ...,  0.2798,  0.1227, -0.1291]])),\n",
              "             ('linear3.bias',\n",
              "              tensor([-0.1349,  0.1772, -0.0380,  0.0647,  0.0166,  0.1594,  0.0326,  0.0499,\n",
              "                      -0.1550, -0.0564]))])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CXI70r7OpI8w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}