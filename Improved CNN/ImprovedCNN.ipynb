{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqxx8KmGonAWP+opJtotJm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaganFoundr/PyTorchNN/blob/main/Improved%20CNN/ImprovedCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "H7_vbOLnuBhQ"
      },
      "outputs": [],
      "source": [
        "#1\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as tt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Applying data normalization and data augmentation\n",
        "statistic=((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)) #mean and std deviation\n",
        "train_transform = tt.Compose([tt.RandomCrop(size=32, padding=4, padding_mode='reflect'),\n",
        "                        tt.RandomHorizontalFlip(),\n",
        "                        tt.ToTensor(),\n",
        "                        tt.Normalize(*statistic)]) # normalizing and augmenting the training set\n",
        "\n",
        "test_transform = tt.Compose([tt.ToTensor(),\n",
        "                             tt.Normalize(*statistic)]) #normalising the test set\n",
        "\n",
        "# downloading the datasets and applying the transforms (for a moment training set is completely taken for training without\n",
        "#splitting and testset is taken for validation and also for testing.\n",
        "dataset=CIFAR10(root='./data', download=True, train=True, transform=train_transform)\n",
        "validation_testset=CIFAR10(root='./data', download=True, train=False, transform=test_transform)\n",
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSLUA63nuHh7",
        "outputId": "ac84328e-56bf-4709-e3c8-6a3c15eb2abc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 33.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "images, labels = dataset[1]\n",
        "rgb_images=images.permute(1,2,0).numpy()\n",
        "objectname=dataset.classes[labels]\n",
        "plt.imshow(rgb_images)\n",
        "plt.show()\n",
        "print(\"object number: \", labels)\n",
        "print(\"object name: \", objectname)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "dxaWaMGsuKyx",
        "outputId": "e5c6bc62-e54c-46b4-f5e8-89e852821476"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAl30lEQVR4nO3de3TV9Znv8c8Gky2RZNMQyKUklIuClstURmIOSrmkhOhigVDH21kN1sKAgQ6iVTPLK21PLJ56PRinUwu6KqB0CSxtixck4dACLQgDaMlAJtOEIYnCLPaGREIkv/OHx9RokN8T9uabHd6vtfZaZO8nT55ffoEPv+ydJwHP8zwBAHCe9XA9AADgwkQAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHDiItcDfFFra6sOHz6s5ORkBQIB1+MAAIw8z9Px48eVlZWlHj3OfJ3T5QLo8OHDys7Odj0GAOAc1dbWasCAAWd8PGYBtGzZMj3++OOqr6/X6NGj9eyzz2rs2LFnfb/k5GRJnw6ekpISq/F8+7+G2k3G3lu3+q/94xsbbM3/zzr/tZPusPXu9R+2+t/O9V3ab8rTptYH18y2zRKnrpz0su/aqkNv2Zo3rLPVAz599u/5mcQkgF555RUtXrxYzz//vHJzc/XUU0+poKBAlZWV6t+//1e+72ffdktJSekSAXSJoTZo7H2RqXmSsXtijAaRlGCdxb8eCb1M9V3ha+R86HmR4fPSIyF2gwAGZ3saJSYvQnjiiSc0Z84c3X777briiiv0/PPPKykpSb/61a9i8eEAAHEo6gF06tQp7dy5U/n5+X/7ID16KD8/X1s7+J5Tc3OzIpFIuxsAoPuLegAdOXJEp0+fVnp6erv709PTVV9f/6X60tJShUKhthsvQACAC4PznwMqKSlROBxuu9XW1roeCQBwHkT9RQhpaWnq2bOnGhoa2t3f0NCgjIyML9UHg0EFg9an7wEA8S7qV0CJiYkaM2aMNm7c2HZfa2urNm7cqLy8vGh/OABAnIrJy7AXL16soqIi/f3f/73Gjh2rp556So2Njbr99ttj8eEAAHEoJgF000036aOPPtJDDz2k+vp6/d3f/Z02bNjwpRcmAAAuXAHP8zzXQ3xeJBJRKBTSI3VhXezzhwx/t8V//80vfmQbaOUThmLrDwA2GWprjL0N9T/4ta31L2fa6rXXUHujqbPnvWobJU794Yj/2m8P7GPqfXpIpv/ivftNvXFhC4fDX/nD4s5fBQcAuDARQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJ7rsKh5pgPznY5rhI9QZJ7LW49zkmqr/5w8X+679l6f/wdQ7yVTddaQEAqb64zGaA2AVDwCgSyKAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcucj3AmR0y1NbEbAp0IGm5rX54yH/tezNNrX/9zCLftXXhsKn3dTNu9F27eEYfU+9YingbTPX/WFDsu/YXb1VZxwHOiCsgAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwImA53me6yE+LxKJKBQyrG6JsdSRP/Rd+9979xq7bzfU3m1rvWSJ79LxRbbWm9fY6vXLN/zX7p9mbG6Raar2vMMxmiO27g0ETPVL3/zfvmv3p0039b58zKWmenQv4XBYKSkpZ3ycKyAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAODERa4HiIqcX/suXffubabW4f3+axf82NRax7evNVQPN/VONqzTqz9iai1tMtbXWD9ArNS5HuC8eNxY31Jwj+/aJ72Fpt4vvjzDd23RbetMvRH/uAICADgR9QB65JFHFAgE2t2GD7f97x0A0P3F5Ftw3/zmN/XOO+/87YNc1D2+0wcAiJ6YJMNFF12kjIyMWLQGAHQTMXkO6MCBA8rKytLgwYN12223qaam5oy1zc3NikQi7W4AgO4v6gGUm5urFStWaMOGDSorK1N1dbWuvfZaHT9+vMP60tJShUKhtlt2dna0RwIAdEFRD6DCwkLdeOONGjVqlAoKCvS73/1Ox44d06uvvtphfUlJicLhcNuttrY22iMBALqgmL86oE+fPrrssst08ODBDh8PBoMKBoOxHgMA0MXE/OeATpw4oaqqKmVmZsb6QwEA4kjUA+iee+5RRUWF/vM//1N//OMfdcMNN6hnz5665ZZbov2hAABxLOrfgjt06JBuueUWHT16VP369dM111yjbdu2qV+/frZGVxyTeqb4Kl20JOC77ap1tjFeuafj5646NsTWXGFDbZOp8/EWQ7F1U06Vsb7JNjvOzTRj/TBT9RZT9fdu9b8ma/qMNabefS653VSPrifqAbR69epotwQAdEPsggMAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCciPmvY+i0llqpNdlX6XtHBvpuW7nfOMfIif5rmxJsvasMu+DSbK1V57/0+Ehj74SPjPWG4S077MxyYtm8y7DunR9p+toqtzVvecJ3aShpnql142H/e+YuybLujYvpFyL+P66AAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACe67iqeA1slJfkq3VzlfxWPjhjnyOznv3bLX43N/R2fJCltiK11yH/p6Cm21v9WZPicSNJKw66X92ytbaw7h+KTdYnMFsPfiXFqsjVPMHyNb/G/tkeSkq68xXet5/2Xqfc/3jnXVP+LsnWm+ng0d/4M37WnTrVoxQu/PWsdV0AAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMCJrrsLTv8l6WJ/pW995L/t/irbGE3ltnqTwb4rk2+82tT5O7f6r51n6iyF77bV7xsy0Xftozdcb5zm7Pum/iami+a6jOkJtvomy/K4OuMyxUzD/r1rjFvstq/3X5v2B1Prf3nuPlP99In+dzVe/w8/N/WOpd++6v8v83U3ftd3bSTSyC44AEDXRQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAATnTdXXADEqQe/pZa/fLVfr7bjhviv1aSQvK/g+1PW0yttf+9v/iuHZlr632d/9VUMffdGQHftY94b5h6f/8x/7WW/XjxbJjxa+XofkOxcV2blGSoNe5pzL3Gf22LcQ9g3c9M5dfd6H+j4va1tlFyb/C/O277WtuixrEzvuO/2PI5Oe7vC4UrIACAE+YA2rx5s6ZNm6asrCwFAgGtW7eu3eOe5+mhhx5SZmamevXqpfz8fB04cCBa8wIAuglzADU2Nmr06NFatmxZh48vXbpUzzzzjJ5//nlt375dl1xyiQoKCnTy5MlzHhYA0H2YnwMqLCxUYWFhh495nqennnpKDzzwgKZPny5Jeumll5Senq5169bp5ptvPrdpAQDdRlSfA6qurlZ9fb3y8/Pb7guFQsrNzdXWrVs7fJ/m5mZFIpF2NwBA9xfVAKqvr5ckpaent7s/PT297bEvKi0tVSgUartlZ2dHcyQAQBfl/FVwJSUlCofDbbfa2lrXIwEAzoOoBlBGRoYkqaGhod39DQ0NbY99UTAYVEpKSrsbAKD7i2oADRo0SBkZGdq4cWPbfZFIRNu3b1deXl40PxQAIM6ZXwV34sQJHTx4sO3t6upq7d69W6mpqcrJydGiRYv0k5/8RJdeeqkGDRqkBx98UFlZWZoxY0Y05wYAxDlzAO3YsUMTJ05se3vx4sWSpKKiIq1YsUL33nuvGhsbNXfuXB07dkzXXHONNmzYoIsvvtj0ccLvL4y7b8dNN2wG+bT+8tgMEsdqmmz1v7o/NnPEs1Cmrb7esoqn7oiteU4vQ7G/1Vt/Y+hdV2drbf0kNq33XTp2xjhT67nzhxt6f8vU2zK3kgzn5xN/ZeYAmjBhgjzPO+PjgUBAS5Ys0ZIlS6ytAQAXEOevggMAXJgIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAE+ZVPEAs5SS5niD+hRNsO9Wamlr8F+/daxsmN9dQbN0F5/+LZd+L75k6z33ItpTwj94P/Re3lJt6f2u44fNi7K0k/5/D/xFY47vW5yo4roAAAG4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJ1jFA3QzkbRMU32LavwXHzHUSpLChtqQsbf/NTIj9tt6t8i2iqdp9jO+a5NWlJp6X5m0xX9xwhBT76bZJb5rDQubdNpnHVdAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACXbBwcy2JUs6YniHHP/rvXAGYycON9X/5hn/+93Ce22zhCx75pRja67/8F860bYj7Rcr60z1+97zXztWR0y9x07MNVTbelvm/oWh7wlJ433UcQUEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAONFlV/GEQiHXI3QxVxrr0wy1bxl7XxiumLLMd+3jT99p6n2dbVuOzfBMU/l3rzcUm/9aWlbDWPcwJfgv/cEdps7fmmJb3aOWsK3eYsh3DMW7TK3Hrp3hvzjB/8mPHD8ljVh11jqugAAAThBAAAAnzAG0efNmTZs2TVlZWQoEAlq3bl27x2fPnq1AINDuNnXq1GjNCwDoJswB1NjYqNGjR2vZsjN/f3zq1Kmqq6tru61adfbvBQIALizmFyEUFhaqsLDwK2uCwaAyMjI6PRQAoPuLyXNA5eXl6t+/v4YNG6b58+fr6NGjZ6xtbm5WJBJpdwMAdH9RD6CpU6fqpZde0saNG/Wzn/1MFRUVKiws1OnTpzusLy0tVSgUartlZ2dHeyQAQBcU9Z8Duvnmm9v+PHLkSI0aNUpDhgxReXm5Jk+e/KX6kpISLV68uO3tSCRCCAHABSDmL8MePHiw0tLSdPDgwQ4fDwaDSklJaXcDAHR/MQ+gQ4cO6ejRo8rMtP10NgCgezN/C+7EiRPtrmaqq6u1e/dupaamKjU1VY8++qhmzZqljIwMVVVV6d5779XQoUNVUFAQ1cEBAPEt4HmeZ3mH8vJyTZw48Uv3FxUVqaysTDNmzNCuXbt07NgxZWVlacqUKfrxj3+s9PR0X/0jkQh74DpkXR6233flM3f/m611UoupfFeV/1mSMi077KRlP59rqK4x9Y6lZ944YKpfeP1Q/8V7bXvp1FTnv9b6d3O4YV+bbF9Xtt1xw4y9PzbW9zLU1hp7f8tQa9sFJ1meb/f/OYlETioUelThcPgrn1YxXwFNmDBBX5VZb775prUlAOACxC44AIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAnzLrhYu6B2wV3/Q9+l0x5caGq967GXfNfWrl1i6o0ve9v/ujtJ0reMq/0sG/JeCgRMvS0b1W4+e0k7n7yZ4794yqPG7mf+TctfNtjYO2yst8xi/GLRWEPtn4y9LV+IfX1XRiIfKxS686y74LgCAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJy4yPUAF7T3Nvku/V1Zi6n16XVlvmtfemicqfdvNpWb6n+35be+aydMmWLqfd2Nt/iuzZ0yxNT7R/c87Lt24Q/+l6l32vBLTPV9Det1/tvUObYCBTW+a0frdlPv63P91153o22914grbbuSQrmZ/ouTrKvGLF+3f7C1btriuzS8vc53baTxE191XAEBAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnAp7nea6H+LxIJKJQyLorCefix//abKr/6UNPmOpP1pWY6i8EB/9i+2s39HL/u+DQvZz66L981yb2+3oMJ7ELh8NKSUk54+NcAQEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOXOR6APg10VS913vXd+0I4yTXD7/fVH/ltazi+aJvTZrjegTEiefXJbgeIWa4AgIAOGEKoNLSUl111VVKTk5W//79NWPGDFVWVrarOXnypIqLi9W3b1/17t1bs2bNUkNDQ1SHBgDEP1MAVVRUqLi4WNu2bdPbb7+tlpYWTZkyRY2NjW01d911l15//XWtWbNGFRUVOnz4sGbOnBn1wQEA8c30HNCGDRvavb1ixQr1799fO3fu1Pjx4xUOh/XCCy9o5cqVmjRpkiRp+fLluvzyy7Vt2zZdffXV0ZscABDXzuk5oHA4LElKTU2VJO3cuVMtLS3Kz89vqxk+fLhycnK0devWDns0NzcrEom0uwEAur9OB1Bra6sWLVqkcePGacSIT19HVV9fr8TERPXp06ddbXp6uurr6zvsU1paqlAo1HbLzs7u7EgAgDjS6QAqLi7Wvn37tHr16nMaoKSkROFwuO1WW1t7Tv0AAPGhUz8HtGDBAr3xxhvavHmzBgwY0HZ/RkaGTp06pWPHjrW7CmpoaFBGRkaHvYLBoILBYGfGAADEMdMVkOd5WrBggdauXat3331XgwYNavf4mDFjlJCQoI0bN7bdV1lZqZqaGuXl5UVnYgBAt2C6AiouLtbKlSu1fv16JScntz2vEwqF1KtXL4VCId1xxx1avHixUlNTlZKSooULFyovL49XwAEA2jEFUFlZmSRpwoQJ7e5fvny5Zs+eLUl68skn1aNHD82aNUvNzc0qKCjQc889F5VhAQDdR8DzPM/1EJ8XiUQUCoX04h/DSuqd4ut90kL++ycl2eZpavFfaxhDklRX4792r6FWko7u939aE/bXmXqvWrnKVF+te0z16EimoTbN2NtSP8TYe5OhtsrYG11dOBxWSsqZ/x1nFxwAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgRKd+HcP5UDT1Z1LgYl+1xT+8w3ffujrb2pnX1qz3Xxzeb+qdKv+z/HTJo6be99040Xfts5N+YupdzcoUByxft7avcRvLah3gq3EFBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnAh4nue5HuLzIpGIQqGQ6zG6nI8abacpLcl/7Z8K7jH1zn3r56b6uHXNK/5rt9wUuzkQB0YaavcaexcZal809o7l3FI4HFZKSsoZH+cKCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHDiItcDnNkg+c/HI4a+TcY5LGuBcoy9r/RduXTdX02dxyrsu3bVXsvnT5IMe34k2T/nsXKrrXxvZWzGwBnMN9aXxWSKzrGvqfHPul7HIpZznx1XQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwImA53me6yE+LxKJKBQKac7PDyuxV4qv9xlm2E2WkxAwzdMy0n/tEP+r3SRJTfv9127adNjUO/fKLN+1CQmm1vrHH79rqv/33062fYAuI9NQWxezKS4cw431hr9AcCIcDisl5cz/jnMFBABwwhRApaWluuqqq5ScnKz+/ftrxowZqqxsvzF4woQJCgQC7W7z5s2L6tAAgPhnCqCKigoVFxdr27Ztevvtt9XS0qIpU6aosbGxXd2cOXNUV1fXdlu6dGlUhwYAxD/T7wPasGFDu7dXrFih/v37a+fOnRo/fnzb/UlJScrIyIjOhACAbumcngMKhz/9pWepqant7n/55ZeVlpamESNGqKSkRE1NZ/6FZM3NzYpEIu1uAIDur9O/EbW1tVWLFi3SuHHjNGLEiLb7b731Vg0cOFBZWVnas2eP7rvvPlVWVuq1117rsE9paakeffTRzo4BAIhTnQ6g4uJi7du3T1u2bGl3/9y5c9v+PHLkSGVmZmry5MmqqqrSkCFDvtSnpKREixcvbns7EokoOzu7s2MBAOJEpwJowYIFeuONN7R582YNGDDgK2tzc3MlSQcPHuwwgILBoILBYGfGAADEMVMAeZ6nhQsXau3atSovL9egQYPO+j67d++WJGVmWn6oDwDQ3ZkCqLi4WCtXrtT69euVnJys+vp6SVIoFFKvXr1UVVWllStX6rrrrlPfvn21Z88e3XXXXRo/frxGjRoVkwMAAMQnUwCVlZVJ+vSHTT9v+fLlmj17thITE/XOO+/oqaeeUmNjo7KzszVr1iw98MADURsYANA9dNldcFmDrlWPHv7y8VDVphhOlOO7MjnnRlPnR35wne/apCT/++4kad7dV5vqLQID77S9Q01ZbAYB4pL/f1MkaYByfdce0nbjLDXGeht2wQEAuiQCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgRJddxXPb1WOUeJG/VTzLt1jXT3QNVyQN9137QdP+GE4CxCP/f3+uSPO/zkaSckIhU33IUp9g662WXobeHxt7h32XfvYbsP34pLVZ71T/C6t4AABdEwEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOOFv2ZoDmxqS1KNHVxgvx3dlcs6Nps53/OA637VJSUmm3vPuvtpUbxEYeKftHWrKYjMILnD+9yN+cMS2S/GDI9ZZLPz/myJJA+R/j90hWfdi1hjro4srIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMCJrrDrpkPXL3hFib1SfNUOk/81NTkJAdMcLSP91w650tRaTYbtIJs2HTb1fnuL/9qEBFNrXTbyu6b6f4/bVTyZhtq6mE1x4RhurLet1+k6bOtvDjlelxNLXAEBAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnuuwuuH+9e5z85+MRQ+cm4yQhQ22Osbf/5XE/evkBU+dwzR7ftavuecLU+9/r1pjqu45bbeUhw26y8EO23ujARGN9vO6Cw2e4AgIAOGEKoLKyMo0aNUopKSlKSUlRXl6efv/737c9fvLkSRUXF6tv377q3bu3Zs2apYaGhqgPDQCIf6YAGjBggB577DHt3LlTO3bs0KRJkzR9+nS9//77kqS77rpLr7/+utasWaOKigodPnxYM2fOjMngAID4ZnoOaNq0ae3e/ulPf6qysjJt27ZNAwYM0AsvvKCVK1dq0qRJkqTly5fr8ssv17Zt23T11VdHb2oAQNzr9HNAp0+f1urVq9XY2Ki8vDzt3LlTLS0tys/Pb6sZPny4cnJytHXr1jP2aW5uViQSaXcDAHR/5gDau3evevfurWAwqHnz5mnt2rW64oorVF9fr8TERPXp06ddfXp6uurr68/Yr7S0VKFQqO2WnZ1tPggAQPwxB9CwYcO0e/dubd++XfPnz1dRUZE++OCDTg9QUlKicDjcdqutre10LwBA/DD/HFBiYqKGDh0qSRozZoz+/Oc/6+mnn9ZNN92kU6dO6dixY+2ughoaGpSRkXHGfsFgUMFg0D45ACCunfPPAbW2tqq5uVljxoxRQkKCNm7c2PZYZWWlampqlJeXd64fBgDQzZiugEpKSlRYWKicnBwdP35cK1euVHl5ud58802FQiHdcccdWrx4sVJTU5WSkqKFCxcqLy+PV8ABAL7EFEAffvihvve976murk6hUEijRo3Sm2++qe985zuSpCeffFI9evTQrFmz1NzcrIKCAj333HOdHK26k+8XbZY1P5ZaSXrPd+W9M/7V1DktyX9tzotppt6v1VnXGXUVK23lI1/xX7vF1hodKXM9wDkYaajda+xdZKh90dg7lnOfnSmAXnjhha98/OKLL9ayZcu0bNmycxoKAND9sQsOAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOCEeRt2rHme53qELum48Rf1JX7iv/bEJ83GaS4Qn8TryiGcf6dj2PtUDHvHcu6z/3se8LrYv/iHDh3il9IBQDdQW1urAQMGnPHxLhdAra2tOnz4sJKTkxUIBNruj0Qiys7OVm1trVJSUhxOGFscZ/dxIRyjxHF2N9E4Ts/zdPz4cWVlZalHjzM/09PlvgXXo0ePr0zMlJSUbn3yP8Nxdh8XwjFKHGd3c67HGQqFzlrDixAAAE4QQAAAJ+ImgILBoB5++GEFg0HXo8QUx9l9XAjHKHGc3c35PM4u9yIEAMCFIW6ugAAA3QsBBABwggACADhBAAEAnIibAFq2bJm+8Y1v6OKLL1Zubq7+9Kc/uR4pqh555BEFAoF2t+HDh7se65xs3rxZ06ZNU1ZWlgKBgNatW9fucc/z9NBDDykzM1O9evVSfn6+Dhw44GbYc3C245w9e/aXzu3UqVPdDNtJpaWluuqqq5ScnKz+/ftrxowZqqysbFdz8uRJFRcXq2/fvurdu7dmzZqlhoYGRxN3jp/jnDBhwpfO57x58xxN3DllZWUaNWpU2w+b5uXl6fe//33b4+frXMZFAL3yyitavHixHn74Yb333nsaPXq0CgoK9OGHH7oeLaq++c1vqq6uru22ZcsW1yOdk8bGRo0ePVrLli3r8PGlS5fqmWee0fPPP6/t27frkksuUUFBgU6ePHmeJz03ZztOSZo6dWq7c7tq1arzOOG5q6ioUHFxsbZt26a3335bLS0tmjJlihobG9tq7rrrLr3++utas2aNKioqdPjwYc2cOdPh1HZ+jlOS5syZ0+58Ll261NHEnTNgwAA99thj2rlzp3bs2KFJkyZp+vTpev/99yWdx3PpxYGxY8d6xcXFbW+fPn3ay8rK8kpLSx1OFV0PP/ywN3r0aNdjxIwkb+3atW1vt7a2ehkZGd7jjz/edt+xY8e8YDDorVq1ysGE0fHF4/Q8zysqKvKmT5/uZJ5Y+fDDDz1JXkVFhed5n567hIQEb82aNW01f/nLXzxJ3tatW12Nec6+eJye53nf/va3vX/6p39yN1SMfO1rX/N++ctfntdz2eWvgE6dOqWdO3cqPz+/7b4ePXooPz9fW7dudThZ9B04cEBZWVkaPHiwbrvtNtXU1LgeKWaqq6tVX1/f7ryGQiHl5uZ2u/MqSeXl5erfv7+GDRum+fPn6+jRo65HOifhcFiSlJqaKknauXOnWlpa2p3P4cOHKycnJ67P5xeP8zMvv/yy0tLSNGLECJWUlKipKX5/dcfp06e1evVqNTY2Ki8v77yeyy63jPSLjhw5otOnTys9Pb3d/enp6dq/f7+jqaIvNzdXK1as0LBhw1RXV6dHH31U1157rfbt26fk5GTX40VdfX29JHV4Xj97rLuYOnWqZs6cqUGDBqmqqkr//M//rMLCQm3dulU9e/Z0PZ5Za2urFi1apHHjxmnEiBGSPj2fiYmJ6tOnT7vaeD6fHR2nJN16660aOHCgsrKytGfPHt13332qrKzUa6+95nBau7179yovL08nT55U7969tXbtWl1xxRXavXv3eTuXXT6ALhSFhYVtfx41apRyc3M1cOBAvfrqq7rjjjscToZzdfPNN7f9eeTIkRo1apSGDBmi8vJyTZ482eFknVNcXKx9+/bF/XOUZ3Om45w7d27bn0eOHKnMzExNnjxZVVVVGjJkyPkes9OGDRum3bt3KxwO6ze/+Y2KiopUUVFxXmfo8t+CS0tLU8+ePb/0CoyGhgZlZGQ4mir2+vTpo8suu0wHDx50PUpMfHbuLrTzKkmDBw9WWlpaXJ7bBQsW6I033tCmTZva/dqUjIwMnTp1SseOHWtXH6/n80zH2ZHc3FxJirvzmZiYqKFDh2rMmDEqLS3V6NGj9fTTT5/Xc9nlAygxMVFjxozRxo0b2+5rbW3Vxo0blZeX53Cy2Dpx4oSqqqqUmZnpepSYGDRokDIyMtqd10gkou3bt3fr8yp9+lt/jx49Glfn1vM8LViwQGvXrtW7776rQYMGtXt8zJgxSkhIaHc+KysrVVNTE1fn82zH2ZHdu3dLUlydz460traqubn5/J7LqL6kIUZWr17tBYNBb8WKFd4HH3zgzZ071+vTp49XX1/verSoufvuu73y8nKvurra+8Mf/uDl5+d7aWlp3ocffuh6tE47fvy4t2vXLm/Xrl2eJO+JJ57wdu3a5f31r3/1PM/zHnvsMa9Pnz7e+vXrvT179njTp0/3Bg0a5H388ceOJ7f5quM8fvy4d88993hbt271qqurvXfeece78sorvUsvvdQ7efKk69F9mz9/vhcKhbzy8nKvrq6u7dbU1NRWM2/ePC8nJ8d79913vR07dnh5eXleXl6ew6ntznacBw8e9JYsWeLt2LHDq66u9tavX+8NHjzYGz9+vOPJbe6//36voqLCq66u9vbs2ePdf//9XiAQ8N566y3P887fuYyLAPI8z3v22We9nJwcLzEx0Rs7dqy3bds21yNF1U033eRlZmZ6iYmJ3te//nXvpptu8g4ePOh6rHOyadMmT9KXbkVFRZ7nffpS7AcffNBLT0/3gsGgN3nyZK+ystLt0J3wVcfZ1NTkTZkyxevXr5+XkJDgDRw40JszZ07c/eepo+OT5C1fvryt5uOPP/buvPNO72tf+5qXlJTk3XDDDV5dXZ27oTvhbMdZU1PjjR8/3ktNTfWCwaA3dOhQ70c/+pEXDofdDm70/e9/3xs4cKCXmJjo9evXz5s8eXJb+Hje+TuX/DoGAIATXf45IABA90QAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJ/4f3hhKWFZJJ5EAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "object number:  9\n",
            "object name:  truck\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E1NhEXWuN2R",
        "outputId": "462b1303-4da6-4385-e901-c4aad2206647"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "batch_size=100\n",
        "\n",
        "#num_workers are used to leverage multiple cpu cores and loading the images in parallel\n",
        "#pin_memory avoids repeated allocation and deallocation of memory by using the same portion of memory(RAM) for loading each batch of data\n",
        "#this is possible only because all of our images are 32x32 pixels\n",
        "training_loader=DataLoader(dataset, batch_size,shuffle=True, num_workers=2, pin_memory=True)\n",
        "validation_loader=DataLoader(validation_testset, batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "print(len(training_loader.dataset))  # For training dataset\n",
        "print(len(validation_loader.dataset))  # For test dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66hqp049uQe_",
        "outputId": "42dd8b5f-5f26-43e2-bb71-a737e20e98cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "#defining the model, here we are gonna use a larger model, wideresnet22 having 22 convolutional layers, 1 key changes to our\n",
        "# model is the addition of residual block in which the inputs get added back to the output feature map obtained by the passing\n",
        "# of inputs through one or more convolutional layers\n",
        "# also applying batch normalization after each convolutional layer to reduce loss and increase the accuracy very fast.\n",
        "\n",
        "#conv2d function\n",
        "def conv2d(inp, out, ks=3, stride=1):\n",
        "  return nn.Conv2d(in_channels=inp,\n",
        "                   out_channels=out,\n",
        "                   kernel_size=ks,\n",
        "                   stride=stride,\n",
        "                   padding=ks//2,\n",
        "                   bias=False)\n",
        "\n",
        "# the order we need to follow is apply batchnormalization, relu function, conv2d layer\n",
        "def batch_relu_conv2d(inp, out):\n",
        "  return nn.Sequential(nn.BatchNorm2d(inp),\n",
        "                       nn.ReLU(inplace=True),\n",
        "                       conv2d(inp,out))\n",
        "\n",
        "#things happening inside a residual block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, inp, out, stride=1):\n",
        "        super().__init__()\n",
        "        self.batchnorm = nn.BatchNorm2d(inp)\n",
        "        self.conv1 = nn.Conv2d(inp, out, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.conv2 = batch_relu_conv2d(out, out)\n",
        "\n",
        "        # Define shortcut branch\n",
        "        if inp != out or stride != 1:\n",
        "            # Match both channels and spatial dimensions\n",
        "            self.shortcut = nn.Conv2d(inp, out, kernel_size=1, stride=stride, bias=False)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()  # No change if dimensions match exactly\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.batchnorm(x), inplace=True)\n",
        "        r = self.shortcut(x)  # Ensure `r` matches dimensions of `x`\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x) * 0.2\n",
        "        return x + r\n",
        "\n",
        "\n",
        "#making groups / blocks\n",
        "def make_blocks(N,inp, out, stride):\n",
        "  start=ResidualBlock(inp, out, stride)\n",
        "  rest = [ResidualBlock(out, out) for j in range(1,N)]\n",
        "  return [start]+rest\n",
        "\n",
        "#flattening class\n",
        "class Flatten(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x.view(x.size(0),-1)\n",
        "\n",
        "#wide resnet class\n",
        "class WideResNet(nn.Module):\n",
        "  def __init__(self, n_groups, N, n_classes, kernel=1, n_start=16):\n",
        "    super().__init__()\n",
        "    #increase channels to n_start using conv layer\n",
        "    layers=[conv2d(3,n_start)]\n",
        "    n_channels = [n_start]\n",
        "\n",
        "    #add groups of basic blocks(increase channels and downsample)\n",
        "    for i in range(n_groups):\n",
        "      n_channels.append(n_start*(2**i)*kernel)\n",
        "      stride=2 if i>0 else 1\n",
        "      layers+=make_blocks(N, n_channels[i], n_channels[i+1], stride)\n",
        "\n",
        "    #pool, flatten and add linear layers for classification\n",
        "    layers+=[nn.BatchNorm2d(n_channels[3]),\n",
        "             nn.ReLU(),\n",
        "             nn.AdaptiveAvgPool2d(1),\n",
        "             nn.Flatten(),\n",
        "             nn.Linear(n_channels[3], n_classes)]\n",
        "\n",
        "    self.features = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.features(x)\n",
        "\n",
        "def wrn_22():\n",
        "    return WideResNet(n_groups=3,N=3,n_classes=10,kernel=6)\n",
        "\n",
        "model = wrn_22()"
      ],
      "metadata": {
        "id": "VhWMnhRWu9NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in training_loader:\n",
        "  prediction = model(images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "KiTOJcmT2i5A",
        "outputId": "9cd0b1f9-6aa7-4418-c697-a173d917e6ff"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2c6c58549270>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-9d93714c61a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrn_22\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-9d93714c61a0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure `r` matches dimensions of `x`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(images.shape)\n",
        "print(prediction.shape)"
      ],
      "metadata": {
        "id": "d1Zm67po2u4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}