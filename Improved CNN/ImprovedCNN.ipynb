{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPEXZUy+VemHCqn62Wraxqw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaganFoundr/PyTorchNN/blob/main/Improved%20CNN/ImprovedCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H7_vbOLnuBhQ"
      },
      "outputs": [],
      "source": [
        "#1\n",
        "import torch\n",
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as tt\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Applying data normalization and data augmentation\n",
        "statistic=((0.4914, 0.4822, 0.4465),(0.2023, 0.1994, 0.2010)) #mean and std deviation\n",
        "train_transform = tt.Compose([tt.RandomCrop(size=32, padding=4, padding_mode='reflect'),\n",
        "                        tt.RandomHorizontalFlip(),\n",
        "                        tt.ToTensor(),\n",
        "                        tt.Normalize(*statistic)]) # normalizing and augmenting the training set\n",
        "\n",
        "test_transform = tt.Compose([tt.ToTensor(),\n",
        "                             tt.Normalize(*statistic)]) #normalising the test set\n",
        "\n",
        "# downloading the datasets and applying the transforms (for a moment training set is completely taken for training without\n",
        "#splitting and testset is taken for validation and also for testing.\n",
        "dataset=CIFAR10(root='./data', download=True, train=True, transform=train_transform)\n",
        "validation_testset=CIFAR10(root='./data', download=True, train=False, transform=test_transform)\n",
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSLUA63nuHh7",
        "outputId": "8145069f-9a65-49ae-de15-bef99213bfd8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3\n",
        "images, labels = dataset[1]\n",
        "rgb_images=images.permute(1,2,0).numpy()\n",
        "objectname=dataset.classes[labels]\n",
        "plt.imshow(rgb_images)\n",
        "plt.show()\n",
        "print(\"object number: \", labels)\n",
        "print(\"object name: \", objectname)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "dxaWaMGsuKyx",
        "outputId": "75e00525-7452-451e-b5cc-b9aaf6fd164a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "object number:  9\n",
            "object name:  truck\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E1NhEXWuN2R",
        "outputId": "3a5cb072-ba1a-4e04-f3df-a9ac68bfd22f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4\n",
        "batch_size=256\n",
        "\n",
        "#num_workers are used to leverage multiple cpu cores and loading the images in parallel\n",
        "#pin_memory avoids repeated allocation and deallocation of memory by using the same portion of memory(RAM) for loading each batch of data\n",
        "#this is possible only because all of our images are 32x32 pixels\n",
        "training_loader=DataLoader(dataset, batch_size,shuffle=True, num_workers=8, pin_memory=True)\n",
        "validation_loader=DataLoader(validation_testset, batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
        "print(len(training_loader.dataset))  # For training dataset\n",
        "print(len(validation_loader.dataset))  # For test dataset\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66hqp049uQe_",
        "outputId": "a719d1a0-3b12-4798-f5ba-57d69b77c53f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n",
            "10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5\n",
        "#defining the model, here we are gonna use a larger model, wideresnet22 having 22 convolutional layers, 1 key changes to our\n",
        "# model is the addition of residual block in which the inputs get added back to the output feature map obtained by the passing\n",
        "# of inputs through one or more convolutional layers\n",
        "# also applying batch normalization after each convolutional layer to reduce loss and increase the accuracy very fast.\n",
        "\n",
        "#conv2d function\n",
        "def Conv_2d(in_channels, out_channels, kernel=3, stride=1):\n",
        "  return nn.Conv2d(in_channels=in_channels,\n",
        "                   out_channels=out_channels,\n",
        "                   kernel_size=kernel,\n",
        "                   stride=stride,\n",
        "                   padding=kernel//2,\n",
        "                   bias=False)\n",
        "\n",
        "# the order we need to follow is apply batchnormalization, relu function, conv2d layer\n",
        "def batch_relu_conv2d(in_channels, out_channels):\n",
        "  return nn.Sequential(nn.BatchNorm2d(in_channels),\n",
        "                       nn.ReLU(inplace=True),\n",
        "                       Conv_2d(in_channels,out_channels))\n",
        "\n",
        "#things happening inside a residual block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.batchnorm = nn.BatchNorm2d(in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.conv2 = batch_relu_conv2d(out_channels, out_channels)\n",
        "\n",
        "        # Define shortcut branch\n",
        "        if in_channels != out_channels or stride != 1:\n",
        "            # Match both channels and spatial dimensions\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()  # No change if dimensions match exactly\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.batchnorm(x), inplace=True)\n",
        "        r = self.shortcut(x)  # Ensure `r` matches dimensions of `x`\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x) * 0.2\n",
        "        return x + r\n",
        "\n",
        "\n",
        "#making groups / blocks\n",
        "def make_blocks(N,in_channels, out_channels, stride):\n",
        "  start=ResidualBlock(in_channels, out_channels, stride)\n",
        "  rest = [ResidualBlock(out_channels, out_channels) for j in range(1,N)]\n",
        "  return [start]+rest\n",
        "\n",
        "#flattening class\n",
        "class Flatten(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x.view(x.size(0),-1)\n",
        "\n",
        "#wide resnet class\n",
        "class WideResNet(nn.Module):\n",
        "  def __init__(self, n_groups, N, n_classes, k=1, n_start=16):\n",
        "    super().__init__()\n",
        "    #increase channels to n_start using conv layer\n",
        "    layers=[Conv_2d(3,n_start)]\n",
        "    n_channels = [n_start]\n",
        "\n",
        "    #add groups of basic blocks(increase channels and downsample)\n",
        "    for i in range(n_groups):\n",
        "      n_channels.append(n_start*(2**i)*k)\n",
        "      stride=2 if i>0 else 1\n",
        "      layers+=make_blocks(N, n_channels[i], n_channels[i+1], stride)\n",
        "\n",
        "    #pool, flatten and add linear layers for classification\n",
        "    layers+=[nn.BatchNorm2d(n_channels[3]),\n",
        "             nn.ReLU(),\n",
        "             nn.AdaptiveAvgPool2d(1),\n",
        "             nn.Flatten(),\n",
        "             nn.Linear(n_channels[3], n_classes)]\n",
        "\n",
        "    self.features = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.features(x)\n",
        "\n",
        "def wrn_22():\n",
        "    return WideResNet(n_groups=3,N=3,n_classes=10,k=6)\n",
        "\n",
        "model = wrn_22()"
      ],
      "metadata": {
        "id": "VhWMnhRWu9NX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, labels in training_loader:\n",
        "  prediction = model(images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "KiTOJcmT2i5A",
        "outputId": "6bd6522c-887e-45b3-af76-ee09acaaee2f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2c6c58549270>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-580dc4641ed0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrn_22\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-580dc4641ed0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure `r` matches dimensions of `x`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2810\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2813\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(images.shape)\n",
        "print(prediction.shape)"
      ],
      "metadata": {
        "id": "d1Zm67po2u4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}